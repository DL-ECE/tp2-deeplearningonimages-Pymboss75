{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "convolution.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "R2JALA0rnGE1",
        "q7Ucg7xRQDwn"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DL-ECE/tp2-deeplearningonimages-Pymboss75/blob/master/convolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeT-9bRdonUj"
      },
      "source": [
        "# TP-2 Deep Learning on Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll9cmbS4QIjW"
      },
      "source": [
        "## Clothes images classification using Fashion-MNIST dataset\n",
        "\n",
        "In this notebook you will train your second and even third neural network. \n",
        "\n",
        "Feel free to look back at the Lecture-2 slides to complete the cells below.\n",
        "\n",
        "\n",
        "\n",
        "All the dependencies are installed. Below we import them and will be using them in all our notebooks.\n",
        "Please feel free to look arround and look at their API.\n",
        "The student should be limited to these imports to complete this work.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8HwsnlpWABC"
      },
      "source": [
        "# Import the different module we will need in this notebook\n",
        "import os\n",
        "\n",
        "# To read and compute on Images: imageio [imageio doc](https://imageio.readthedocs.io/en/stable/)\n",
        "# To create some plot and figures: matplolib [matplotlib doc](https://matplotlib.org/)\n",
        "# To do computation on matrix and vectors: numpy [numpy doc](https://numpy.org/)\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# To do computation on matrix and vectors and automatic differenciation: pytorch [torch doc](https://pytorch.org/docs/stable/index.html)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# To do some computation on images with pytorch direclty on the GPU [torchvision doc](https://pytorch.org/vision)\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST, FashionMNIST\n",
        "import random\n",
        "import tqdm.notebook as tq\n",
        "\n",
        "# To get the same data as TP1 \n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "# enable tpu computation\n",
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6hnJJcPSJcu"
      },
      "source": [
        "# In order to have some reproducable results and easier debugging \n",
        "# we fix the seed of random.\n",
        "random.seed(1342)\n",
        "np.random.seed(1342)\n",
        "torch.manual_seed(1342)\n",
        "torch.cuda.manual_seed_all(1342)\n",
        "\n",
        "import builtins as __builtin__\n",
        "def print(*args, **kwargs):\n",
        "    \"\"\"My custom print() function.\"\"\"\n",
        "    return __builtin__.print(*args, **kwargs, end='\\n\\n')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2JALA0rnGE1"
      },
      "source": [
        "## Refresh on numpy and images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_6UGR8EUgqi",
        "outputId": "00c30b0e-fb44-487f-95ac-0c06d48b5f48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Let's do again basics of numpy \n",
        "mat_numpy = np.arange(15).reshape(3, 5)\n",
        "print(mat_numpy) # Create a vector from 0 to 14 and reshape it into a Matrix 3X5\n",
        "\n",
        "print(mat_numpy.shape) # Return the size of the matrix (3, 5)\n",
        "\n",
        "print(mat_numpy[0]) # Return the first row of the matrix \n",
        "\n",
        "print(mat_numpy[0,3]) # Return first row and 4th column  element \n",
        "\n",
        "# Also interesting with higher dimension \n",
        "# Below can be though of 2 3X4 matrix \n",
        "tensor = np.zeros((2,3,4))   # Create an tensor of shape [2,2,2] of all zeros\n",
        "print(tensor)                # Prints [[[0. 0. 0. 0.]\n",
        "                             #          [0. 0. 0. 0.]\n",
        "                             #          [0. 0. 0. 0.]]\n",
        "                             #        [[0. 0. 0. 0.]\n",
        "                             #         [0. 0. 0. 0.]\n",
        "                             #         [0. 0. 0. 0.]]]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2  3  4]\n",
            " [ 5  6  7  8  9]\n",
            " [10 11 12 13 14]]\n",
            "\n",
            "(3, 5)\n",
            "\n",
            "[0 1 2 3 4]\n",
            "\n",
            "3\n",
            "\n",
            "[[[0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHj2OBNMw5VA"
      },
      "source": [
        "Now it's your turn create a function that return a tensor of shape \n",
        "n_rowsxn_columsxn_channels that contains a default value every where"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR0fNMzPwtem"
      },
      "source": [
        "def build_image_like_tensor(n_rows:int, n_colums: int, n_channels:int, default_value: int)-> np.ndarray:\n",
        "  \"\"\"Create a tensor of 3 dimension. \n",
        "     It should have a shape similar to (n_rows, n_colums, n_channels)\n",
        "     It should be containing the default value set by default_value\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  tensor = np.zeros((n_rows,n_colums,n_channels))\n",
        "  tensor[tensor == 0] = default_value\n",
        "  return tensor"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYPMAOSdxi8S"
      },
      "source": [
        "# Create 3 different tensors with the above function containing different value between [0,255]\n",
        "# Uncomment the 3 line below and complete with your answer \n",
        "\n",
        "white_like = build_image_like_tensor(16,16,3,1)\n",
        "gray_like = build_image_like_tensor(16,16,3,0.5)\n",
        "black_like = build_image_like_tensor(16,16,3,0)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI1sR5lWyTvG"
      },
      "source": [
        "# Each of the tensor that you have created can be seen as an image. Use here is the way to display it using matplotlib imshow:\n",
        "def plot_one_tensor(image_tensor: np.array):\n",
        "    \"\"\"Function to plot the image tensor\"\"\"\n",
        "    plt.imshow(image_tensor, cmap='gray')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMKHtF-FnGFP",
        "outputId": "e4f89f63-e438-4f03-bb17-6aca16a8e454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_one_tensor(white_like)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8nnjpziXJyo",
        "outputId": "3127e4ef-2300-4a49-dc91-4d5e7a42a38f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_one_tensor(gray_like)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ6-IuvlnGFW",
        "outputId": "264f6746-f961-4556-9c68-6747b435b052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_one_tensor(black_like)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncTl3AjcnGFb"
      },
      "source": [
        "We saw that an digital image is the combination of a 3 channel tensor RGB. \n",
        "Each channel represent respectively the R red componant, G greed componant, B blue componant. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUtptCwanGFc"
      },
      "source": [
        "# Create again 3 image tensors with your function\n",
        "# Then change them to be representing a red, a green, a blue image\n",
        "# Uncomment the 3 line below and complete with your answer \n",
        "\n",
        "\n",
        "red_like = build_image_like_tensor(16,16,3,0)\n",
        "red_like[:,:,0] = 1\n",
        "green_like = build_image_like_tensor(16,16,3,0)\n",
        "green_like[:,:,1] = 1\n",
        "blue_like = build_image_like_tensor(16,16,3,0)\n",
        "blue_like[:,:,2] = 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jXLgMmfnGFh",
        "outputId": "dc013311-101a-462b-86f7-aaca3d43491d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_one_tensor(red_like)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL0iMGb6nGFl",
        "outputId": "d8a887e5-8539-4563-ff09-79533b9bccfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_one_tensor(green_like)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOtqQNcCnGFp",
        "outputId": "9276fd9d-d710-4cbb-8483-25a8a01da980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_one_tensor(blue_like)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7Ucg7xRQDwn"
      },
      "source": [
        "## What Pytorch can do\n",
        "\n",
        "*   Similar functions to Numpy on GPU\n",
        "*   Calculate automatically gradient on the neural network\n",
        "*   Some neural networks layers are already coded : dense, convolution, pooling, etc\n",
        "*   Calculate automatically the weights update\n",
        "*   Provide optimizer to compute gradient descent \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjeKBFJHXYl8",
        "outputId": "6b7d05f0-3a87-44c4-a1e8-85f1f8bb3671",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mat_torch = torch.arange(15).reshape(3,5)\n",
        "\n",
        "print(mat_torch) # Create a vector from 0 to 14 and reshape it into a Matrix 3X5\n",
        "print(mat_torch.shape) # Return the size of the matrix (3, 5)\n",
        "print(mat_torch[0]) # Return the first row of the matrix \n",
        "print(mat_torch[0,3]) # Return first row and 4th column element \n",
        "# This was easy but everything was on the CPU so it's the same as Numpy \n",
        "# To do computation on the GPU (graphic card calculation can be 50x faster)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14]])\n",
            "\n",
            "torch.Size([3, 5])\n",
            "\n",
            "tensor([0, 1, 2, 3, 4])\n",
            "\n",
            "tensor(3)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4fBZlMHYMqI",
        "outputId": "0379a874-d7f2-4ca4-aa4f-741fce16d0f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# What is the GPU on this machine ? \n",
        "# !nvidia-smi\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujP5e7pGYLCh",
        "outputId": "37a7a9a1-238c-4754-d235-a75dcb9514cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mat_torch = torch.arange(15, device=device).reshape(3,5)\n",
        "print(mat_torch) # Create a vector from 0 to 14 and reshape it into a Matrix 3X5\n",
        "print(mat_torch.shape) # Return the size of the matrix (3, 5)\n",
        "print(mat_torch[0]) # Return the first row of the matrix \n",
        "print(mat_torch[0,3]) # Return first row and 4th column element "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14]], device='cuda:0')\n",
            "\n",
            "torch.Size([3, 5])\n",
            "\n",
            "tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
            "\n",
            "tensor(3, device='cuda:0')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YQO8LsGZ8iq"
      },
      "source": [
        "Let's say we want a faster sigmoid and softmax. \n",
        "We can use the same function from TP-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0bUELpAYn0O"
      },
      "source": [
        "def normalize_tensor(input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Apply a normalization to the tensor\"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    return input_tensor/255\n",
        "   \n",
        "\n",
        "def sigmoid(input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Apply a sigmoid to the input Tensor\"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    sig = 1/(1+torch.exp(-input_tensor))\n",
        "    return sig\n",
        "\n",
        "def softmax(input_tensor: torch.Tensor)-> torch.Tensor:\n",
        "    \"\"\"Apply a softmax to the input tensor\"\"\"\n",
        "    # YOUR CODE HERE \n",
        "    fct = nn.softmax(dim=1)\n",
        "    return fct(input_tensor)\n",
        "\n",
        "def target_to_one_hot(targets: torch.Tensor, num_classes=10) -> torch.Tensor:\n",
        "    \"\"\"Create the one hot representation of the target\"\"\" \n",
        "    # YOUR CODE HERE \n",
        "    one_hot_matrix = torch.zeros([targets.shape[0], num_classes])\n",
        "    for i in range(targets.shape[0]):\n",
        "      one_hot_matrix[i][int(targets[i])] = 1\n",
        "    return one_hot_matrix"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Je20wNZuj0"
      },
      "source": [
        "# However as mention above pytorch already has some built-ins function \n",
        "\n",
        "# sigmoid function [sigmoid doc](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html?highlight=sigmoid#torch.nn.Sigmoid)\n",
        "# softmax function [softmax doc](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html?highlight=softmax#torch.nn.Softmax) \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXpmI-kTbq4T"
      },
      "source": [
        "mat_torch = torch.arange(15, dtype=torch.float64, device=device).reshape(3,5)\n",
        "# Uncomment the line bellow to check if your implementation is correct\n",
        "\n",
        "# assert torch.allclose(sigmoid(mat_torch), torch.sigmoid(mat_torch))\n",
        "# print(sigmoid(mat_torch))\n",
        "# print(torch.sigmoid(mat_torch))\n",
        "\n",
        "# assert torch.allclose(sotfmax(mat_torch))\n",
        "# print(softmax(mat_torch))\n",
        "# print(torch.softmax(mat_torch, dim=1))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp405FXquGqz"
      },
      "source": [
        "## Transforming our Neural network from TP1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCnZAR7x2yUl"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Downloading again the same MNIST dataset \n",
        "\n",
        "    mnist_data, mnist_target = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(mnist_data, mnist_target, test_size=0.33, random_state=1342)\n",
        "    # Change the input data to be normalize and target data to be correctly encoded \n",
        "\n",
        "    X_train = normalize_tensor(X_train)\n",
        "    X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "\n",
        "    X_test = normalize_tensor(X_test)\n",
        "    X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "\n",
        "    y_train = target_to_one_hot(y_train)\n",
        "    #y_train = torch.from_numpy(y_train).long()\n",
        "\n",
        "    y_test = target_to_one_hot(y_test)\n",
        "    #y_test = torch.from_numpy(y_test).long()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYxuYKhUuT6U"
      },
      "source": [
        "Your remember the famous `class FFNN` from **TP1** ?? \n",
        "\n",
        "Here we will create the same version but with pytorch and we will see the power of this framework. \n",
        "\n",
        "Auto calculation of the backward pass and auto update of the weights ðŸŽ‰ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x677wA4zvtMR"
      },
      "source": [
        "In pytorch a dense layer similar to our `Class Layer` is a called **Linear Layer**\n",
        "\n",
        "[linear layer documentation] -> https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlBuCmXNxIRY"
      },
      "source": [
        "class FFNN(nn.Module):\n",
        "    def __init__(self, config, device, minibatch_size=100, learning_rate=0.01, momentum=0):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        self.nlayers = len(config)\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.device = device \n",
        "\n",
        "        # We use the built-in activation functions\n",
        "        # TODO: Maybe try with another activation function ! \n",
        "        self.activation = torch.nn.Sigmoid()\n",
        "        # self.activation = torch.nn.ReLU()\n",
        "\n",
        "\n",
        "        self.last_activation = torch.nn.Softmax(dim=1)\n",
        "\n",
        "        # First difference we don't need a special Input layer ðŸ˜ƒ\n",
        "        # Second one we can declare them more easely\n",
        "        for i in range(1,len(config)):\n",
        "          layer = nn.Linear(config[i-1], config[i])\n",
        "          self.layers.append(layer)\n",
        "          self.layers.append(self.activation)\n",
        "\n",
        "        self.layers[-1]= self.last_activation\n",
        "        self.model = nn.Sequential(*self.layers)\n",
        "\n",
        "        # We use the built-in function to compute the loss\n",
        "        # TODO: Maybe try with another loss function ! \n",
        "        self.loss_function = torch.nn.MSELoss()\n",
        "        # self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # We use the built-in function to update the model weights\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=self.momentum)\n",
        "\n",
        "    # Here we see the power of Pytorch\n",
        "    # The forward is just giving the input to our model\n",
        "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "      y_pred = self.model(input_tensor)\n",
        "      return y_pred\n",
        "\n",
        "    def compute_loss(self, y_pred: torch.Tensor, y_true) -> torch.Tensor:\n",
        "        y_true = torch.argmax(y_true, dim=1)\n",
        "        loss = self.loss_function(y_pred.float(), y_true)\n",
        "        # looking at what the loss looks like\n",
        "        # print(loss)\n",
        "        return loss\n",
        "\n",
        "    # Even more powerful no need to code all the derivative of the different function\n",
        "    def backward_pass(self, loss: torch.tensor) -> None:\n",
        "        loss.backward()\n",
        "        return\n",
        "\n",
        "    # The previoulsy hard function to update the weight become also easy\n",
        "    def update_all_weights(self):\n",
        "      # Using pytorch\n",
        "      self.optimizer.step()\n",
        "\n",
        "\n",
        "    def get_error(self, y_pred, y_true) -> float:\n",
        "      y_pred = torch.argmax(y_pred, dim=1)\n",
        "      y_true = torch.argmax(y_true, dim=1)\n",
        "      return (y_pred == y_true).float().mean()\n",
        "\n",
        "    def get_test_error(self, X_test, y_test) -> float:\n",
        "      nbatch = X_test.shape[0]\n",
        "      error_sum = 0.0\n",
        "      for i in range(0, nbatch):\n",
        "          X_batch = X_test[i,:,:].reshape(self.minibatch_size, -1)\n",
        "          y_batch = y_test[i,:,:].reshape(self.minibatch_size, -1)\n",
        "          y_pred = self.model(X_batch)\n",
        "          error_sum += self.get_error(y_pred, y_batch)\n",
        "      return error_sum / nbatch\n",
        "\n",
        "    def train(self, n_epochs: int, X_train: torch.Tensor, y_train: torch.Tensor, X_test: torch.Tensor, y_test: torch.Tensor):\n",
        "      X_train = X_train.reshape(-1, self.minibatch_size, 784).to(self.device)\n",
        "      y_train = y_train.reshape(-1, self.minibatch_size, 10).to(self.device)\n",
        "\n",
        "      X_test = X_test.reshape(-1, self.minibatch_size, 784).to(self.device)\n",
        "      y_test = y_test.reshape(-1, self.minibatch_size, 10).to(self.device)\n",
        "\n",
        "      \n",
        "      self.model = self.model.to(device)\n",
        "      nbatch = X_train.shape[0]\n",
        "      error_test = 0.0\n",
        "      for epoch in range(n_epochs): \n",
        "        error_sum_train = 0.0\n",
        "        for i in range(0, nbatch):\n",
        "          X_batch = X_train[i,:, :]\n",
        "          y_batch = y_train[i,:, :]\n",
        "          # In order to have the correct derivative we remove the one from before \n",
        "          self.optimizer.zero_grad()\n",
        "          # Then we do a pass forward \n",
        "          y_pred = self.model(X_batch)\n",
        "          # We compute the loss \n",
        "          loss = self.compute_loss(y_pred, y_batch)\n",
        "          # And calculate the backward pass\n",
        "          self.backward_pass(loss=loss)\n",
        "          # To finally update the weights using stochastic gradient descent \n",
        "          self.update_all_weights()\n",
        "          error_sum_train += self.get_error(y_pred, y_batch)\n",
        "        error_test = self.get_test_error(X_test, y_test)\n",
        "        \n",
        "        print(f\"Training Loss: {loss:.3f}, Training accuracy: {error_sum_train / nbatch:.3f}, Test accuracy: {error_test:.3f}\")\n",
        "      return loss, error_test"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAzW9AJMzyOq",
        "tags": [],
        "outputId": "dfc2a495-451b-4f16-80ca-b22879f20fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    minibatch_size = 28\n",
        "    nepoch = 50\n",
        "    learning_rate = 0.1\n",
        "    ffnn = FFNN(config=[784, 256, 128, 10], device=device, minibatch_size=minibatch_size, learning_rate=learning_rate)\n",
        "    print(ffnn)\n",
        "    loss, err = ffnn.train(nepoch, X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN(\n",
            "  (activation): Sigmoid()\n",
            "  (last_activation): Softmax(dim=1)\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (3): Sigmoid()\n",
            "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
            "    (5): Softmax(dim=1)\n",
            "  )\n",
            "  (loss_function): MSELoss()\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-4aa6ea34d055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mffnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFFNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-86287e3ebe5b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     95\u001b[0m           \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m           \u001b[0;31m# We compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m           \u001b[0;31m# And calculate the backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-86287e3ebe5b>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;31m# looking at what the loss looks like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (28) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTQ_8e8gK3RA"
      },
      "source": [
        "In pytorch a very convinient way to load data in batch si to use the data loader. \n",
        "\n",
        "Let's update the class to use it, we are also going to use dataset available in pytorch vision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei5R8mqlxOZi"
      },
      "source": [
        "class FFNNModel(nn.Module):\n",
        "    def __init__(self, classes=10):\n",
        "        super().__init__()\n",
        "        # not the best model...\n",
        "        self.l1 = torch.nn.Linear(784, 256)\n",
        "        self.l2 = torch.nn.Linear(256, 128)\n",
        "        self.l3 = torch.nn.Linear(128, classes)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.last_activation = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input.reshape(input.size(0), -1)\n",
        "        x = self.l1(input)\n",
        "        x = self.activation(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.l3(x)\n",
        "        y = self.last_activation(x)\n",
        "        return y\n",
        "\n",
        "def train_one_epoch(model, device, data_loader, optimizer):\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    for num, (data, target) in tq.tqdm(enumerate(data_loader), total=len(data_loader.dataset)/data_loader.batch_size):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        prediction = output.argmax(dim=1)\n",
        "        correct += torch.sum(prediction.eq(target)).item()\n",
        "\n",
        "    result = {'loss': train_loss / len(data_loader.dataset),\n",
        "              'accuracy': correct / len(data_loader.dataset)\n",
        "              }\n",
        "    return result   \n",
        " \n",
        "def evaluation(model, device, data_loader):\n",
        "    eval_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for num, (data, target) in tq.tqdm(enumerate(data_loader), total=len(data_loader.dataset)/data_loader.batch_size):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        eval_loss += F.cross_entropy(output, target).item()\n",
        "        prediction = output.argmax(dim=1)\n",
        "        correct += torch.sum(prediction.eq(target)).item()\n",
        "    result = {'loss': eval_loss / len(data_loader.dataset),\n",
        "              'accuracy': correct / len(data_loader.dataset)\n",
        "              }\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcj3lBVPgeIN"
      },
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # Network Hyperparameters \n",
        "    minibatch_size = 28\n",
        "    nepoch = 10\n",
        "    learning_rate = 0.1\n",
        "    momentum = 0 \n",
        "    model = FFNNModel()\n",
        "    model.to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "    # Retrieve the data with the pytorch dataloader \n",
        "    mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n",
        "    mnist_train = DataLoader(mnist_train, batch_size=32, num_workers=4, pin_memory=True)\n",
        "    mnist_val = MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())\n",
        "    mnist_val = DataLoader(mnist_val, batch_size=32, num_workers=4,  pin_memory=True)\n",
        "\n",
        "    # Train for an number of epoch \n",
        "    for epoch in range(nepoch):\n",
        "      print(f\"training Epoch: {epoch}\")\n",
        "      if epoch > 0:\n",
        "        train_result = train_one_epoch(model, device, mnist_train, optimizer)\n",
        "        print(f\"Result Training dataset {train_result}\")\n",
        "\n",
        "      eval_result = evaluation(model, device, mnist_val)\n",
        "      print(f\"Result Test dataset {eval_result}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR0RUMI0WABc"
      },
      "source": [
        "# Part 1: What is a convolution ?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q06D0V9KnzJv"
      },
      "source": [
        "In this section you will implement 2D convolution operation using:\n",
        "\n",
        "Starting with a simple example and manual computation like in Lecture 2\n",
        "\n",
        "1) Introduction: manual computation\n",
        "\n",
        "- you have as input an image of 5x5 pixels\n",
        "\n",
        "$I = \\begin{bmatrix}I_{1, 1} & ... & I_{1, 5} \\\\ \\vdots & \\ddots & \\vdots \\\\ I_{5, 1}& ... & I_{5,5}\\end{bmatrix}$\n",
        "\n",
        "Your task is to compute the result of a convolution operation between this image and a 3x3 kernel\n",
        "\n",
        "$ K = \\begin{bmatrix}a & b & c \\\\d & e & f \\\\ g& h& i\\end{bmatrix}$\n",
        "\n",
        "We are considering padding with 0 and using the SAME convolution. \n",
        "Meaning that arround the I matrix consider there is the value 0.\n",
        "\n",
        "Tips: the result of the convolution is a 5x5 matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfvn5c9yWABe"
      },
      "source": [
        "I = np.array([[252,  49, 113,  11, 137],\n",
        "                [ 18, 237, 163, 119,  53],\n",
        "                [ 90,  89, 178,  75, 247],\n",
        "                [209, 216,  48, 135, 232],\n",
        "                [229, 53, 107, 106, 222]])\n",
        "print(f\"I =\")\n",
        "print(I)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDhH5cxzWABl"
      },
      "source": [
        "K_0 = np.array([[0, 1, 0], [0, 0, 0], [0, 0, 0]])\n",
        "print(f\"K_0 =\")\n",
        "print(K_0)\n",
        "\n",
        "K_1 = np.array([[1, 1, 1], [0, 5, 0], [-1, -1, -1]])\n",
        "print(f\"K_1 =\")\n",
        "print(K_1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqaSC3rTWABq"
      },
      "source": [
        "What is the result of convolution of $ I_0 \\ast K_0 $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WidbhmkRWABr"
      },
      "source": [
        "# put your answer here\n",
        "R_0 = np.array([0,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfi2u2yVWABw"
      },
      "source": [
        "What is the result of convolution of $ I_0 \\ast K_1 $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XKt_u-wWABx"
      },
      "source": [
        "# put your answer here\n",
        "R_1 = np.array([0,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu-2xPRZWAB0"
      },
      "source": [
        "## 2) Computation using __numpy__\n",
        "\n",
        "Now using the numpy implement the convolution operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "gP7fnMBHetJg"
      },
      "source": [
        "def convolution_forward_numpy(image, kernel):\n",
        "    # YOUR CODE HERE \n",
        "    NotImplemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OGXKtyVetJm"
      },
      "source": [
        "Test your implementation on the two previous example and compare the results to the result manually computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRubH1y6WAB1",
        "lines_to_next_cell": 2
      },
      "source": [
        "\n",
        "# assert convolution_forward_numpy(I, K_0) == R_0\n",
        "# assert convolution_forward_numpy(I, K_1) == R_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Lmj8tf3WACI"
      },
      "source": [
        "Display the result image of the convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoEYVPzFWACR",
        "lines_to_next_cell": 2
      },
      "source": [
        "# Load image from url, you can use an other image if you want\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/4/4f/ECE_Paris_Lyon.jpg\"\n",
        "image = imageio.imread(image_url)\n",
        "\n",
        "\n",
        "# simple function to display image\n",
        "def display_image(img):\n",
        "    plt.imshow(img)\n",
        "\n",
        "# display the image\n",
        "display_image(image)\n",
        "\n",
        "\n",
        "# Do the convolution operation and display the resulting image\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# output_image = convolution_forward_numpy(image, kernel) \n",
        "# display_image(output_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7z9q-rtWACQ"
      },
      "source": [
        "## 3) Computation using __pytorch__\n",
        "\n",
        "Now let's use pytorch convolution layer to do the forward pass. Use the documentation available at: https://pytorch.org/docs/stable/nn.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZboYqQCWACW",
        "lines_to_next_cell": 2
      },
      "source": [
        "def convolution_forward_torch(image, kernel):\n",
        "    # YOUR CODE HERE \n",
        "    NotImplemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9XtW00JWACZ"
      },
      "source": [
        "In pytorch you can also access other layer like convolution2D, pooling layers, for example in the following cell use the __torch.nn.MaxPool2d__ to redduce the image size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEpb5XVyWACf",
        "lines_to_next_cell": 2
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5aVluRsoVMC"
      },
      "source": [
        "# Part 2: Using convolution neural network to recognize digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YkGWHXIWACi"
      },
      "source": [
        "In this section you will implement 2D convolution neural network and train it on fashion mnist dataset\n",
        "\n",
        "https://github.com/zalandoresearch/fashion-mnist\n",
        "\n",
        "\n",
        "![Image of fashion mnist](https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/doc/img/fashion-mnist-sprite.png)\n",
        "\n",
        "##  First let's look at the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfBxOSnDIwle"
      },
      "source": [
        "if __name__ == \"__main__\" :\n",
        "\n",
        "  fmnist_train = FashionMNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n",
        "  fmnist_train = DataLoader(fmnist_train, batch_size=32, num_workers=4, pin_memory=True)\n",
        "  fmnist_val = FashionMNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())\n",
        "  fmnist_val = DataLoader(fmnist_val, batch_size=32, num_workers=4,  pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWhl5or3WACl"
      },
      "source": [
        "Display the 10 image from train set and 10 images from validation set, print their ground truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfbE7VKIIpOD"
      },
      "source": [
        "def display_10_images(dataset):\n",
        "    # YOUR CODE HERE \n",
        "    NotImplemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNl2tW4OWACm"
      },
      "source": [
        "What is the shape of each images\n",
        "How many images do we have\n",
        "What are the different classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ex3UohlH0o6"
      },
      "source": [
        "def fashion_mnist_dataset_answer():\n",
        "    shape = None  # replace None with the value you found\n",
        "    number_of_images_in_train_set = None\n",
        "    number_of_images_in_test_set = None\n",
        "    number_of_classes = None\n",
        "    return {'shape': shape, 'nb_in_train_set': number_of_images_in_train_set, 'nb_in_test_set': number_of_images_in_test_set, 'number_of_classes': number_of_classes}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCt97SpbI7pO"
      },
      "source": [
        "# Plot an image and the target  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHK65RunWADA"
      },
      "source": [
        "## Create a convolutional neural network\n",
        "\n",
        "Now it's your turn to create a convolutional neural network and to train your model on the fashion mnist dataset.\n",
        "\n",
        "Classical machine learning approach manage to get a 89% accuracy on fashion mnist, your objective is to use deep learning (and convolution neural network) to get more than 90%\n",
        "\n",
        "You can first start with this simple convolution network and improve it by adding/modifying the layers used:\n",
        "\n",
        "```\n",
        "convolutional layer 3x3\n",
        "convolutional layer 3x3\n",
        "max-pooling\n",
        "convolutional layer 3x3\n",
        "convolutional layer 3x3\n",
        "max-pooling\n",
        "flatten\n",
        "fully-connected layer (dense layer)\n",
        "fully-connected layer (dense layer)\n",
        "fully-connected layer (dense layer)\n",
        "Softmax\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W7t-is0WADA",
        "lines_to_next_cell": 2
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, classes=10):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE \n",
        "        self.conv1 = NotImplemented\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        # YOUR CODE HERE \n",
        "        y = NotImplemented\n",
        "        return y\n",
        "\n",
        "def train_one_epoch(model, device, data_loader, optimizer):\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    for num, (data, target) in tq.tqdm(enumerate(data_loader), total=len(data_loader.dataset)/data_loader.batch_size):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "        # YOUR CODE HERE \n",
        "        loss = NotImplemented\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        prediction = output.argmax(dim=1)\n",
        "        correct += torch.sum(prediction.eq(target)).item()\n",
        "\n",
        "    result = {'loss': train_loss / len(data_loader.dataset),\n",
        "              'accuracy': correct / len(data_loader.dataset)\n",
        "              }\n",
        "    return result   \n",
        " \n",
        "def evaluation(model, device, data_loader):\n",
        "    eval_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for num, (data, target) in tq.tqdm(enumerate(data_loader), total=len(data_loader.dataset)/data_loader.batch_size):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # YOUR CODE HERE \n",
        "        eval_loss = NotImplemented\n",
        "        prediction = output.argmax(dim=1)\n",
        "        correct += torch.sum(prediction.eq(target)).item()\n",
        "    result = {'loss': eval_loss / len(data_loader.dataset),\n",
        "              'accuracy': correct / len(data_loader.dataset)\n",
        "              }\n",
        "    return result\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # Network Hyperparameters \n",
        "    # YOUR CODE HERE \n",
        "    minibatch_size = NotImplemented\n",
        "    nepoch = NotImplemented\n",
        "    learning_rate = NotImplemented\n",
        "    momentum = NotImplemented\n",
        "\n",
        "\n",
        "    model = CNNModel()\n",
        "    model.to(device)\n",
        "\n",
        "    # YOUR CODE HERE \n",
        "    optimizer = NotImplemented\n",
        "\n",
        "    # Train for an number of epoch \n",
        "    for epoch in range(nepoch):\n",
        "      print(f\"training Epoch: {epoch}\")\n",
        "      if epoch > 0:\n",
        "        train_result = train_one_epoch(model, device, fmnist_train, optimizer)\n",
        "        print(f\"Result Training dataset {train_result}\")\n",
        "\n",
        "      eval_result = evaluation(model, device, mnist_val)\n",
        "      print(f\"Result Test dataset {eval_result}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urVNNdXIIpOP"
      },
      "source": [
        "## Open Analysis\n",
        "Same as TP 1 please write a short description of your experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkn7C1QrIpOP"
      },
      "source": [
        "# BONUS \n",
        "\n",
        "Use some already trained CNN to segment YOUR image. \n",
        "\n",
        "In the cell below your can load a image to the notebook and use the given network to have the segmentation mask and plot it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EsvJUIuIpOQ"
      },
      "source": [
        "if __name__ == \"__main__\" :\n",
        "    \n",
        "    # TODO HERE: Upload an image to the notebook in the navigation bar on the left\n",
        "    # `File` `Load File`and load an image to the notebook. \n",
        "    \n",
        "    filename = \"\" \n",
        "    # Loading a already trained network in pytorch \n",
        "    model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True)\n",
        "    model.eval()\n",
        "\n",
        "    from PIL import Image\n",
        "    from torchvision import transforms\n",
        "\n",
        "    input_image = Image.open(filename)\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    input_tensor = preprocess(input_image)\n",
        "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "    # move the input and model to GPU for speed if available\n",
        "    if torch.cuda.is_available():\n",
        "        input_batch = input_batch.to('cuda')\n",
        "        model.to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_batch)['out'][0]\n",
        "    output_predictions = output.argmax(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC-PrnerIpOU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}